{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Dynamic Pricing: A Complete Tutorial\n",
    "\n",
    "This notebook provides a comprehensive, self-contained tutorial on applying reinforcement learning to dynamic pricing problems. All code, explanations, and visualizations are included in this single notebook for easy sharing and reference.\n",
    "\n",
    "## Business Context\n",
    "\n",
    "Online retailers face several key challenges in pricing strategy:\n",
    "\n",
    "1. **Variable Demand**: Customer demand fluctuates based on price, seasonality, and random factors\n",
    "2. **Competitive Market**: Competitors constantly adjust their prices based on our pricing strategy\n",
    "3. **Limited Inventory**: Products have finite inventory that must be allocated optimally over time\n",
    "4. **Profit vs. Market Share**: Need to balance immediate profit against long-term market share\n",
    "5. **Seasonal Patterns**: Weekend and holiday effects significantly impact demand\n",
    "\n",
    "Traditional pricing approaches struggle with these dynamics. Reinforcement learning offers a promising solution by learning optimal pricing policies through experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's install and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this if you need to install dependencies\n",
    "# !pip install numpy matplotlib seaborn tensorflow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import deque\n",
    "import warnings\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directories for results if they don't exist\n",
    "os.makedirs('notebook_results', exist_ok=True)\n",
    "os.makedirs('notebook_results/models', exist_ok=True)\n",
    "os.makedirs('notebook_results/visualizations', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define some helper functions we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# Set seeds for reproducibility\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer Implementation\n",
    "\n",
    "The replay buffer is used by DQN and Double DQN agents to store and sample experiences, breaking correlations between consecutive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"Initialize a replay buffer.\"\"\"\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences from the buffer.\"\"\"\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        \n",
    "        for i in indices:\n",
    "            s, a, r, ns, d = self.buffer[i]\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(ns)\n",
    "            dones.append(d)\n",
    "        \n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\"Return the current size of the buffer.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Implementation\n",
    "\n",
    "Now we'll implement the pricing environment. This is where we model all the business aspects:\n",
    "\n",
    "1. Variable demand\n",
    "2. Competitive pricing dynamics\n",
    "3. Limited inventory\n",
    "4. Seasonality effects\n",
    "5. Market share dynamics\n",
    "\n",
    "The `DynamicPricingEnv` class follows the standard RL environment interface with `reset()` and `step()` methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DynamicPricingEnv:\n",
    "    def __init__(self, initial_demand=100, elasticity=0.5, competitor_prices=[15.99, 22.99],\n",
    "                 seasonality_factors={'weekend': 1.2, 'holiday': 1.5}, inventory=1000,\n",
    "                 price_points=None, episode_length=7, market_share_split=0.6):\n",
    "        \"\"\"\n",
    "        Initialize the dynamic pricing environment.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        initial_demand : float\n",
    "            Initial customer demand\n",
    "        elasticity : float\n",
    "            Price elasticity of demand\n",
    "        competitor_prices : list\n",
    "            List of competitor prices [low_price, high_price]\n",
    "        seasonality_factors : dict\n",
    "            Seasonality effects on demand\n",
    "        inventory : int\n",
    "            Initial inventory level\n",
    "        price_points : list\n",
    "            Available price points (if None, will use default)\n",
    "        episode_length : int\n",
    "            Number of days in a pricing period\n",
    "        market_share_split : float\n",
    "            Portion of total market demand we can capture (0-1)\n",
    "        \"\"\"\n",
    "        self.initial_demand = initial_demand\n",
    "        self.current_demand = initial_demand\n",
    "        self.elasticity = elasticity\n",
    "        self.competitor_low_price = min(competitor_prices)\n",
    "        self.competitor_high_price = max(competitor_prices)\n",
    "        self.current_competitor_price = self.competitor_low_price  # Start with low price\n",
    "        self.seasonality_factors = seasonality_factors\n",
    "        self.inventory = inventory\n",
    "        self.day = 0\n",
    "        self.price_history = []\n",
    "        self.competitor_price_history = []\n",
    "        self.episode_length = episode_length\n",
    "        self.market_share_split = market_share_split\n",
    "        \n",
    "        # Default price points if none provided\n",
    "        self.price_points = price_points if price_points is not None else [9.99, 14.99, 19.99, 24.99, 29.99]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state.\"\"\"\n",
    "        self.current_demand = self.initial_demand\n",
    "        self.day = 0\n",
    "        self.price_history = []\n",
    "        self.competitor_price_history = []\n",
    "        self.inventory = 1000\n",
    "        self.current_competitor_price = self.competitor_low_price  # Start with low price\n",
    "        return self._get_state()\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        action : int\n",
    "            Index of the price point to use\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        next_state : array\n",
    "            Next state observation\n",
    "        reward : float\n",
    "            Profit earned\n",
    "        done : bool\n",
    "            Whether the episode is complete\n",
    "        info : dict\n",
    "            Additional information\n",
    "        \"\"\"\n",
    "        # Get price from action\n",
    "        price = self.price_points[action]\n",
    "        \n",
    "        # Update competitor price strategically based on our price\n",
    "        self._update_competitor_price(price)\n",
    "        \n",
    "        # Calculate the median price point as a threshold\n",
    "        median_price = np.median(self.price_points)\n",
    "        \n",
    "        # Calculate total market demand based on seasonality\n",
    "        seasonality = self._get_seasonality()\n",
    "        total_market_demand = self.current_demand * seasonality\n",
    "        \n",
    "        # Calculate how price-competitive we are compared to competitor\n",
    "        relative_price = price / self.current_competitor_price\n",
    "        price_effect = np.exp(-self.elasticity * (relative_price - 1))\n",
    "        \n",
    "        # Calculate our market share based on pricing and base split\n",
    "        # If our price is equal to competitor, we get our base market share\n",
    "        # If our price is higher, we get less; if lower, we get more\n",
    "        our_market_share = self.market_share_split * price_effect\n",
    "        our_market_share = max(0.1, min(0.9, our_market_share))  # Cap between 10-90%\n",
    "        \n",
    "        # Our demand is based on our share of the total market\n",
    "        potential_demand = total_market_demand * our_market_share\n",
    "        \n",
    "        # Constrained by inventory\n",
    "        demand = min(potential_demand, self.inventory)\n",
    "        \n",
    "        # Calculate profit and update inventory\n",
    "        sales = int(demand)\n",
    "        profit = sales * price\n",
    "        self.inventory -= sales\n",
    "        \n",
    "        # Update demand with some randomness\n",
    "        self.current_demand = max(20, self.current_demand + np.random.normal(-5, 10))\n",
    "        \n",
    "        # Update day and price history\n",
    "        self.day += 1\n",
    "        self.price_history.append(price)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = (self.day >= self.episode_length or self.inventory <= 0)\n",
    "        \n",
    "        # Additional info\n",
    "        info = {\n",
    "            'price': price,\n",
    "            'sales': sales,\n",
    "            'demand': demand,\n",
    "            'inventory': self.inventory,\n",
    "            'competitor_price': self.current_competitor_price,\n",
    "            'market_share': our_market_share\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), profit, done, info\n",
    "        \n",
    "    def _update_competitor_price(self, our_price):\n",
    "        \"\"\"Update competitor price based on our pricing strategy.\"\"\"\n",
    "        # Calculate the median price point as a reference threshold\n",
    "        median_price = np.median(self.price_points)\n",
    "        \n",
    "        # If our price is high, competitor is more likely to price low to steal market share\n",
    "        if our_price > median_price:\n",
    "            # Higher our price, higher probability competitor prices low\n",
    "            prob_low = min(0.8, 0.5 + 0.1 * (our_price - median_price))\n",
    "        else:\n",
    "            # If our price is low, competitor is less likely to go even lower\n",
    "            prob_low = max(0.2, 0.5 - 0.1 * (median_price - our_price))\n",
    "        \n",
    "        # Determine competitor price based on probability\n",
    "        if np.random.random() < prob_low:\n",
    "            self.current_competitor_price = self.competitor_low_price\n",
    "        else:\n",
    "            self.current_competitor_price = self.competitor_high_price\n",
    "            \n",
    "        # Store competitor price in history\n",
    "        self.competitor_price_history.append(self.current_competitor_price)\n",
    "    \n",
    "    def _get_seasonality(self):\n",
    "        \"\"\"Get seasonality factor for current day.\"\"\"\n",
    "        if self.day % 7 >= 5:  # Weekend\n",
    "            return self.seasonality_factors.get('weekend', 1.2)\n",
    "        elif self.day % 30 >= 28:  # End of month\n",
    "            return self.seasonality_factors.get('holiday', 1.5)\n",
    "        return 1.0\n",
    "        \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get current state representation.\"\"\"\n",
    "        return np.array([\n",
    "            self.day % 7,  # Day of week\n",
    "            self.current_demand,\n",
    "            self.inventory,\n",
    "            self.current_competitor_price,  # Current competitor price\n",
    "            self.competitor_low_price,      # Competitor's low price strategy\n",
    "            self._get_seasonality()\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Environment\n",
    "\n",
    "Let's test our pricing environment by running a few steps and observing the behavior:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create environment\n",
    "env = DynamicPricingEnv()\n",
    "\n",
    "# Display environment parameters\n",
    "print(f\"Price points: {env.price_points}\")\n",
    "print(f\"Competitor price range: ${env.competitor_low_price} - ${env.competitor_high_price}\")\n",
    "print(f\"Initial inventory: {env.inventory}\")\n",
    "print(f\"Episode length: {env.episode_length} days\")\n",
    "print(f\"Seasonality factors: {env.seasonality_factors}\")\n",
    "\n",
    "# Initialize environment and get state space dimensions\n",
    "state = env.reset()\n",
    "print(f\"\\nState representation: {state}\")\n",
    "print(f\"State space dimensions: {len(state)}\")\n",
    "print(f\"Action space dimensions: {len(env.price_points)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's simulate a few steps with different pricing strategies:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Try different pricing strategies\n",
    "print(\"Testing different pricing strategies:\")\n",
    "print(\"\\n1. Low price strategy:\")\n",
    "state = env.reset()\n",
    "action = 0  # Lowest price\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"  Selected price: ${env.price_points[action]:.2f}\")\n",
    "print(f\"  Resulting profit: ${reward:.2f}\")\n",
    "print(f\"  Sales: {info['sales']} units\")\n",
    "print(f\"  Competitor price: ${info['competitor_price']:.2f}\")\n",
    "print(f\"  Our market share: {info['market_share']:.2%}\")\n",
    "\n",
    "print(\"\\n2. Medium price strategy:\")\n",
    "state = env.reset()\n",
    "action = 2  # Medium price\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"  Selected price: ${env.price_points[action]:.2f}\")\n",
    "print(f\"  Resulting profit: ${reward:.2f}\")\n",
    "print(f\"  Sales: {info['sales']} units\")\n",
    "print(f\"  Competitor price: ${info['competitor_price']:.2f}\")\n",
    "print(f\"  Our market share: {info['market_share']:.2%}\")\n",
    "\n",
    "print(\"\\n3. High price strategy:\")\n",
    "state = env.reset()\n",
    "action = 4  # Highest price\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"  Selected price: ${env.price_points[action]:.2f}\")\n",
    "print(f\"  Resulting profit: ${reward:.2f}\")\n",
    "print(f\"  Sales: {info['sales']} units\")\n",
    "print(f\"  Competitor price: ${info['competitor_price']:.2f}\")\n",
    "print(f\"  Our market share: {info['market_share']:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a full episode with a fixed medium pricing strategy and visualize the results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run a full episode with fixed pricing\n",
    "state = env.reset()\n",
    "rewards = []\n",
    "prices = []\n",
    "demands = []\n",
    "competitor_prices = []\n",
    "market_shares = []\n",
    "\n",
    "for day in range(env.episode_length):\n",
    "    action = 2  # Medium price\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Record metrics\n",
    "    rewards.append(reward)\n",
    "    prices.append(env.price_points[action])\n",
    "    demands.append(info['demand'])\n",
    "    competitor_prices.append(info['competitor_price'])\n",
    "    market_shares.append(info['market_share'])\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "# Prices and competitor prices\n",
    "axes[0].plot(prices, 'b-o', label='Our Price')\n",
    "axes[0].plot(competitor_prices, 'r-s', label='Competitor Price')\n",
    "axes[0].set_title('Our Price vs. Competitor Price')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Daily profit\n",
    "axes[1].bar(range(len(rewards)), rewards, color='g')\n",
    "axes[1].set_title('Daily Profit')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Profit ($)')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Market share\n",
    "axes[2].plot(market_shares, 'purple', marker='o')\n",
    "axes[2].set_title('Our Market Share')\n",
    "axes[2].set_xlabel('Day')\n",
    "axes[2].set_ylabel('Market Share (%)')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total profit: ${sum(rewards):.2f}\")\n",
    "print(f\"Average market share: {np.mean(market_shares):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementations\n",
    "\n",
    "In this section, we'll implement three reinforcement learning agents for dynamic pricing:\n",
    "\n",
    "1. **Monte Carlo Agent**: Learns from complete episodes without bootstrapping\n",
    "2. **DQN (Deep Q-Network)**: Uses experience replay and a neural network for Q-value approximation\n",
    "3. **Double DQN**: Extends DQN by separating action selection and evaluation to reduce overestimation bias\n",
    "\n",
    "We'll start with a base agent class that defines the common interface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"Initialize base agent with state and action dimensions.\"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.training_history = {'rewards': [], 'losses': [], 'epsilons': []}\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action given the current state.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this!\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent based on experiences.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this!\")\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save agent model.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this!\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load agent model.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Agent\n",
    "\n",
    "The Monte Carlo agent learns directly from complete episodes. It updates Q-values based on the actual returns experienced, which makes it less prone to bootstrapping bias but potentially slower to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class MonteCarloAgent(BaseAgent):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=1.0, \n",
    "                 epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        \"\"\"\n",
    "        Initialize a Monte Carlo agent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        state_dim : int\n",
    "            Dimension of the state space\n",
    "        action_dim : int\n",
    "            Dimension of the action space\n",
    "        learning_rate : float\n",
    "            Learning rate for the neural network\n",
    "        gamma : float\n",
    "            Discount factor\n",
    "        epsilon : float\n",
    "            Initial exploration rate\n",
    "        epsilon_decay : float\n",
    "            Decay rate for exploration\n",
    "        epsilon_min : float\n",
    "            Minimum exploration rate\n",
    "        \"\"\"\n",
    "        super().__init__(state_dim, action_dim)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.episode_memory = []\n",
    "        \n",
    "        # Build neural network model\n",
    "        self.model = self._build_model(learning_rate)\n",
    "        \n",
    "    def _build_model(self, learning_rate):\n",
    "        \"\"\"Build a neural network model for Q-value approximation.\"\"\"\n",
    "        model = Sequential([\n",
    "            Dense(64, input_dim=self.state_dim, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.action_dim, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        state : array\n",
    "            Current state\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        action : int\n",
    "            Selected action\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        q_values = self.model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "        return np.argmax(q_values)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state=None, done=None):\n",
    "        \"\"\"\n",
    "        Store a transition in episode memory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        state : array\n",
    "            Current state\n",
    "        action : int\n",
    "            Action taken\n",
    "        reward : float\n",
    "            Reward received\n",
    "        next_state : array, optional\n",
    "            Next state (not used in Monte Carlo)\n",
    "        done : bool, optional\n",
    "            Whether the episode is done (not used in Monte Carlo)\n",
    "        \"\"\"\n",
    "        self.episode_memory.append((state, action, reward))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent on the completed episode.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Training loss\n",
    "        \"\"\"\n",
    "        if not self.episode_memory:\n",
    "            return 0\n",
    "        \n",
    "        states, actions, rewards = zip(*self.episode_memory)\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        # Normalize returns for stability\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
    "        \n",
    "        # Train on episode data\n",
    "        states = np.array(states)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (state, action, G) in enumerate(zip(states, actions, returns)):\n",
    "            current_q = self.model.predict(state.reshape(1, -1), verbose=0)[0]\n",
    "            target_q = current_q.copy()\n",
    "            target_q[action] = G\n",
    "            \n",
    "            history = self.model.fit(\n",
    "                state.reshape(1, -1), \n",
    "                target_q.reshape(1, -1), \n",
    "                verbose=0\n",
    "            )\n",
    "            total_loss += history.history['loss'][0]\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        # Clear episode memory\n",
    "        self.episode_memory = []\n",
    "        \n",
    "        # Record training history\n",
    "        if hasattr(self, 'training_history'):\n",
    "            if len(states) > 0:\n",
    "                self.training_history['losses'].append(total_loss / len(states))\n",
    "                self.training_history['epsilons'].append(self.epsilon)\n",
    "        \n",
    "        return total_loss / len(states) if len(states) > 0 else 0\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the agent's model to a file.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.model.save(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the agent's model from a file.\"\"\"\n",
    "        self.model = load_model(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent\n",
    "\n",
    "The Deep Q-Network (DQN) agent uses a neural network to approximate Q-values and experience replay to improve stability. It learns from bootstrapped estimates, which can lead to faster learning but potentially higher bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DQNAgent(BaseAgent):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=1.0, \n",
    "                 epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000):\n",
    "        \"\"\"\n",
    "        Initialize a DQN agent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        state_dim : int\n",
    "            Dimension of the state space\n",
    "        action_dim : int\n",
    "            Dimension of the action space\n",
    "        learning_rate : float\n",
    "            Learning rate for the neural network\n",
    "        gamma : float\n",
    "            Discount factor\n",
    "        epsilon : float\n",
    "            Initial exploration rate\n",
    "        epsilon_decay : float\n",
    "            Decay rate for exploration\n",
    "        epsilon_min : float\n",
    "            Minimum exploration rate\n",
    "        batch_size : int\n",
    "            Size of training batch\n",
    "        memory_size : int\n",
    "            Size of replay memory\n",
    "        \"\"\"\n",
    "        super().__init__(state_dim, action_dim)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(memory_size)\n",
    "        \n",
    "        # Build neural network model\n",
    "        self.model = self._build_model(learning_rate)\n",
    "        \n",
    "        # Initialize training history\n",
    "        self.training_history = {\n",
    "            'losses': [],\n",
    "            'epsilons': []\n",
    "        }\n",
    "        \n",
    "    def _build_model(self, learning_rate):\n",
    "        \"\"\"Build a neural network model for Q-value approximation.\"\"\"\n",
    "        model = Sequential([\n",
    "            Dense(64, input_dim=self.state_dim, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.action_dim, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay memory.\"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        state = np.array(state).reshape(1, -1)\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using experience replay (vectorized for speed).\"\"\"\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return 0\n",
    "\n",
    "        # Sample random batch from memory\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # Predict Q-values for all states and next_states in batch\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "        # Compute targets\n",
    "        targets = q_values.copy()\n",
    "        batch_index = np.arange(self.batch_size)\n",
    "        targets[batch_index, actions] = rewards + (1 - dones) * self.gamma * np.max(next_q_values, axis=1)\n",
    "\n",
    "        # Train on the batch\n",
    "        history = self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        loss = history.history['loss'][0]\n",
    "        self.training_history['losses'].append(loss)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.training_history['epsilons'].append(self.epsilon)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the agent's model to a file.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.model.save(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the agent's model from a file.\"\"\"\n",
    "        self.model = load_model(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN Agent\n",
    "\n",
    "The Double DQN agent extends DQN by using two separate networks for action selection and evaluation. This helps reduce the overestimation bias common in standard Q-learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DoubleDQNAgent(BaseAgent):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=1.0, \n",
    "                 epsilon_decay=0.995, epsilon_min=0.01, buffer_size=10000, batch_size=64,\n",
    "                 update_target_every=100):\n",
    "        \"\"\"\n",
    "        Initialize a Double DQN agent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        state_dim : int\n",
    "            Dimension of the state space\n",
    "        action_dim : int\n",
    "            Dimension of the action space\n",
    "        learning_rate : float\n",
    "            Learning rate for the neural network\n",
    "        gamma : float\n",
    "            Discount factor\n",
    "        epsilon : float\n",
    "            Initial exploration rate\n",
    "        epsilon_decay : float\n",
    "            Decay rate for exploration\n",
    "        epsilon_min : float\n",
    "            Minimum exploration rate\n",
    "        buffer_size : int\n",
    "            Size of replay buffer\n",
    "        batch_size : int\n",
    "            Size of training batch\n",
    "        update_target_every : int\n",
    "            Frequency of target network updates\n",
    "        \"\"\"\n",
    "        super().__init__(state_dim, action_dim)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_every = update_target_every\n",
    "        self.training_count = 0\n",
    "        \n",
    "        # Create replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Create main network (for action selection)\n",
    "        self.q_network = self._build_model(learning_rate)\n",
    "        \n",
    "        # Create target network (for action evaluation)\n",
    "        self.target_network = self._build_model(learning_rate)\n",
    "        self.update_target_network()  # Sync weights initially\n",
    "        \n",
    "        # Initialize training history\n",
    "        self.training_history = {\n",
    "            'losses': [],\n",
    "            'epsilons': []\n",
    "        }\n",
    "    \n",
    "    def _build_model(self, learning_rate):\n",
    "        \"\"\"Build a neural network model.\"\"\"\n",
    "        model = Sequential([\n",
    "            Dense(64, input_dim=self.state_dim, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(self.action_dim, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network weights from the main network.\"\"\"\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        state = np.array(state).reshape(1, -1)\n",
    "        q_values = self.q_network.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using Double DQN algorithm.\"\"\"\n",
    "        # If we don't have enough samples in memory, skip training\n",
    "        if self.replay_buffer.size() < self.batch_size:\n",
    "            return 0\n",
    "\n",
    "        # Sample mini-batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Double DQN: Select actions using online network\n",
    "        q_values_next = self.q_network.predict(next_states, verbose=0)\n",
    "        best_actions = np.argmax(q_values_next, axis=1)\n",
    "\n",
    "        # Evaluate actions using target network\n",
    "        target_q_values = self.target_network.predict(next_states, verbose=0)\n",
    "\n",
    "        # Get Q-values for best actions (from online network) using target network\n",
    "        target_q = target_q_values[np.arange(self.batch_size), best_actions]\n",
    "\n",
    "        # Set target using Bellman equation\n",
    "        targets = rewards + (1 - dones) * self.gamma * target_q\n",
    "\n",
    "        # Get current Q-values and update targets for actions taken\n",
    "        current_q = self.q_network.predict(states, verbose=0)\n",
    "        batch_indices = np.arange(self.batch_size)\n",
    "        current_q[batch_indices, actions] = targets\n",
    "\n",
    "        # Train the network\n",
    "        history = self.q_network.fit(states, current_q, verbose=0, epochs=1)\n",
    "        loss = history.history['loss'][0]\n",
    "\n",
    "        # Update target network periodically\n",
    "        self.training_count += 1\n",
    "        if self.training_count % self.update_target_every == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        # Decay epsilon (reduce exploration over time)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Record training history\n",
    "        self.training_history['losses'].append(loss)\n",
    "        self.training_history['epsilons'].append(self.epsilon)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the agent's models to files.\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.q_network.save(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the agent's models from files.\"\"\"\n",
    "        self.q_network = load_model(path)\n",
    "        self.target_network = load_model(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Agents\n",
    "\n",
    "Let's create instances of our three agents to make sure they're functioning correctly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create environment for testing\n",
    "env = DynamicPricingEnv()\n",
    "state = env.reset()\n",
    "state_dim = len(state)\n",
    "action_dim = len(env.price_points)\n",
    "\n",
    "# Initialize all three agent types\n",
    "mc_agent = MonteCarloAgent(state_dim, action_dim)\n",
    "dqn_agent = DQNAgent(state_dim, action_dim)\n",
    "ddqn_agent = DoubleDQNAgent(state_dim, action_dim)\n",
    "\n",
    "print(\"Monte Carlo Agent Architecture:\")\n",
    "print(f\"  Neural Network: 2 hidden layers with 64 units each\")\n",
    "print(f\"  Learning approach: Episode-based learning with no bootstrapping\")\n",
    "print(f\"  Exploration strategy: Epsilon-greedy with decay\")\n",
    "\n",
    "print(\"\\nDQN Agent Architecture:\")\n",
    "print(f\"  Neural Network: 2 hidden layers with 64 units each\")\n",
    "print(f\"  Learning approach: TD learning with experience replay\")\n",
    "print(f\"  Replay buffer size: 10,000 transitions\")\n",
    "print(f\"  Batch size: 32 transitions per update\")\n",
    "\n",
    "print(\"\\nDouble DQN Agent Architecture:\")\n",
    "print(f\"  Neural Networks: Main network and target network with 2 hidden layers each\")\n",
    "print(f\"  Learning approach: TD learning with separated action selection and evaluation\")\n",
    "print(f\"  Replay buffer size: 10,000 transitions\")\n",
    "print(f\"  Batch size: 64 transitions per update\")\n",
    "print(f\"  Target network update frequency: Every 100 training steps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the action selection mechanism for all agents:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test action selection for all agents\n",
    "state = env.reset()\n",
    "\n",
    "# Monte Carlo agent action\n",
    "mc_action = mc_agent.select_action(state)\n",
    "print(f\"Monte Carlo agent selected price: ${env.price_points[mc_action]:.2f}\")\n",
    "\n",
    "# DQN agent action\n",
    "dqn_action = dqn_agent.select_action(state)\n",
    "print(f\"DQN agent selected price: ${env.price_points[dqn_action]:.2f}\")\n",
    "\n",
    "# Double DQN agent action\n",
    "ddqn_action = ddqn_agent.select_action(state)\n",
    "print(f\"Double DQN agent selected price: ${env.price_points[ddqn_action]:.2f}\")\n",
    "\n",
    "# Note: Initially, all agents will make random selections due to high epsilon values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Now that we have our environment and agents implemented, let's develop training and evaluation functions to:\n",
    "\n",
    "1. Train each agent type\n",
    "2. Track performance metrics during training\n",
    "3. Evaluate trained agents on equivalent test scenarios\n",
    "4. Compare performance across different algorithms\n",
    "\n",
    "First, let's define the training function that works with all three agent types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_agent(agent, env, num_episodes=200, max_steps=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Train an agent on the dynamic pricing environment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    agent : BaseAgent\n",
    "        The agent to train\n",
    "    env : DynamicPricingEnv\n",
    "        The environment to train on\n",
    "    num_episodes : int\n",
    "        Number of episodes to train for\n",
    "    max_steps : int\n",
    "        Maximum steps per episode\n",
    "    verbose : bool\n",
    "        Whether to print training progress\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    training_history : dict\n",
    "        Dictionary of training metrics\n",
    "    \"\"\"\n",
    "    # Reset environment to get initial state dimensions\n",
    "    state = env.reset()\n",
    "    state_dim = len(state)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    episode_rewards = []\n",
    "    episode_avg_prices = []\n",
    "    episode_losses = []\n",
    "    \n",
    "    if verbose:\n",
    "        # Use tqdm for a progress bar\n",
    "        episodes = tqdm(range(num_episodes), desc=f\"Training {agent.__class__.__name__}\")\n",
    "    else:\n",
    "        episodes = range(num_episodes)\n",
    "    \n",
    "    # Loop through episodes\n",
    "    for episode in episodes:\n",
    "        # Reset environment and metrics\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        prices = []\n",
    "        losses = []\n",
    "        \n",
    "        # For Monte Carlo, need to store full episode\n",
    "        is_monte_carlo = isinstance(agent, MonteCarloAgent)\n",
    "        \n",
    "        # Loop through steps in episode\n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            if is_monte_carlo:\n",
    "                agent.store_transition(state, action, reward)\n",
    "            else:\n",
    "                agent.store_transition(state, action, reward, next_state, done)\n",
    "                # DQN and Double DQN can train on each step\n",
    "                loss = agent.train()\n",
    "                if loss:\n",
    "                    losses.append(loss)\n",
    "            \n",
    "            # Record metrics\n",
    "            total_reward += reward\n",
    "            prices.append(env.price_points[action])\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # For Monte Carlo, train at end of episode\n",
    "        if is_monte_carlo:\n",
    "            loss = agent.train()\n",
    "            if loss:\n",
    "                losses.append(loss)\n",
    "        \n",
    "        # Record episode metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        if prices:\n",
    "            episode_avg_prices.append(np.mean(prices))\n",
    "        if losses:\n",
    "            episode_losses.append(np.mean(losses))\n",
    "        \n",
    "        # Update progress bar\n",
    "        if verbose and episode % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:]) if episode_rewards else 0\n",
    "            avg_price = np.mean(episode_avg_prices[-10:]) if episode_avg_prices else 0\n",
    "            avg_loss = np.mean(episode_losses[-10:]) if episode_losses else 0\n",
    "            if isinstance(tqdm, type):  # Check if tqdm is available\n",
    "                episodes.set_postfix({\n",
    "                    'reward': f'{avg_reward:.2f}', \n",
    "                    'price': f'{avg_price:.2f}',\n",
    "                    'loss': f'{avg_loss:.4f}'\n",
    "                })\n",
    "    \n",
    "    # Compile training history\n",
    "    training_history = {\n",
    "        'rewards': episode_rewards,\n",
    "        'prices': episode_avg_prices,\n",
    "        'losses': episode_losses\n",
    "    }\n",
    "    \n",
    "    return training_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a function to evaluate trained agents on identical test scenarios to ensure fair comparison:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_agent(agent, env, num_episodes=10, seed=None):\n",
    "    \"\"\"\n",
    "    Evaluate an agent on the dynamic pricing environment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    agent : BaseAgent\n",
    "        The agent to evaluate\n",
    "    env : DynamicPricingEnv\n",
    "        The environment to evaluate on\n",
    "    num_episodes : int\n",
    "        Number of episodes to evaluate for\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Set seeds for reproducibility\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        set_seeds(seed)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    total_rewards = []\n",
    "    episode_prices = []\n",
    "    episode_sales = []\n",
    "    episode_demands = []\n",
    "    episode_competitor_prices = []\n",
    "    episode_market_shares = []\n",
    "    \n",
    "    # Loop through episodes\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        prices = []\n",
    "        sales = []\n",
    "        demands = []\n",
    "        competitor_prices = []\n",
    "        market_shares = []\n",
    "        \n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select action (no exploration during evaluation)\n",
    "            # Use \"select_action\" as it already contains epsilon-greedy logic\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Record metrics\n",
    "            episode_reward += reward\n",
    "            prices.append(info['price'])\n",
    "            sales.append(info['sales'])\n",
    "            demands.append(info['demand'])\n",
    "            competitor_prices.append(info['competitor_price'])\n",
    "            market_shares.append(info['market_share'])\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            \n",
    "            if step >= env.episode_length:\n",
    "                break\n",
    "        \n",
    "        # Record episode metrics\n",
    "        total_rewards.append(episode_reward)\n",
    "        episode_prices.append(prices)\n",
    "        episode_sales.append(sales)\n",
    "        episode_demands.append(demands)\n",
    "        episode_competitor_prices.append(competitor_prices)\n",
    "        episode_market_shares.append(market_shares)\n",
    "    \n",
    "    # Compile evaluation results\n",
    "    results = {\n",
    "        'rewards': total_rewards,\n",
    "        'prices': episode_prices,\n",
    "        'sales': episode_sales,\n",
    "        'demands': episode_demands,\n",
    "        'competitor_prices': episode_competitor_prices,\n",
    "        'market_shares': episode_market_shares,\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training All Agents\n",
    "\n",
    "Now let's train all three agent types on our dynamic pricing environment. We'll use the same environment configuration for fair comparison.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Create environment\n",
    "env = DynamicPricingEnv()\n",
    "state = env.reset()\n",
    "state_dim = len(state)\n",
    "action_dim = len(env.price_points)\n",
    "\n",
    "# Initialize agents\n",
    "mc_agent = MonteCarloAgent(state_dim, action_dim, epsilon=1.0, epsilon_decay=0.99)\n",
    "dqn_agent = DQNAgent(state_dim, action_dim, epsilon=1.0, epsilon_decay=0.99)\n",
    "ddqn_agent = DoubleDQNAgent(state_dim, action_dim, epsilon=1.0, epsilon_decay=0.99)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 300  # Moderate number for demonstration; increase for better performance\n",
    "max_steps = env.episode_length\n",
    "\n",
    "# Train agents\n",
    "print(\"Training Monte Carlo agent...\")\n",
    "mc_history = train_agent(mc_agent, env, num_episodes, max_steps)\n",
    "\n",
    "print(\"Training DQN agent...\")\n",
    "dqn_history = train_agent(dqn_agent, env, num_episodes, max_steps)\n",
    "\n",
    "print(\"Training Double DQN agent...\")\n",
    "ddqn_history = train_agent(ddqn_agent, env, num_episodes, max_steps)\n",
    "\n",
    "# Save trained models\n",
    "mc_agent.save('notebook_results/models/monte_carlo_agent.h5')\n",
    "dqn_agent.save('notebook_results/models/dqn_agent.h5')\n",
    "ddqn_agent.save('notebook_results/models/ddqn_agent.h5')\n",
    "\n",
    "print(\"All agents trained successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Performance\n",
    "\n",
    "Let's visualize the training performance of all three agents to understand their learning dynamics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training rewards\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Smoothing function for clearer visualization\n",
    "def smooth(data, window=10):\n",
    "    \"\"\"Apply moving average smoothing to data.\"\"\"\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    weights = np.ones(window) / window\n",
    "    return np.convolve(data, weights, mode='valid')\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(smooth(mc_history['rewards']), label='Monte Carlo')\n",
    "plt.plot(smooth(dqn_history['rewards']), label='DQN')\n",
    "plt.plot(smooth(ddqn_history['rewards']), label='Double DQN')\n",
    "plt.title('Training Rewards (Smoothed)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(smooth(mc_history['losses']), label='Monte Carlo')\n",
    "plt.plot(smooth(dqn_history['losses']), label='DQN')\n",
    "plt.plot(smooth(ddqn_history['losses']), label='Double DQN')\n",
    "plt.title('Training Losses (Smoothed)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebook_results/visualizations/training_comparison.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Agents on Test Scenarios\n",
    "\n",
    "Now let's evaluate all three trained agents on identical test scenarios to ensure a fair comparison:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluation parameters\n",
    "num_eval_episodes = 10\n",
    "eval_seeds = [100, 200, 300, 400, 500]  # Different seeds for robustness\n",
    "\n",
    "# Evaluate agents across multiple seeds\n",
    "mc_results = []\n",
    "dqn_results = []\n",
    "ddqn_results = []\n",
    "\n",
    "for seed in eval_seeds:\n",
    "    print(f\"Evaluating with seed {seed}...\")\n",
    "    \n",
    "    # Evaluate Monte Carlo\n",
    "    env_mc = DynamicPricingEnv()  # Fresh environment for each evaluation\n",
    "    mc_result = evaluate_agent(mc_agent, env_mc, num_eval_episodes, seed)\n",
    "    mc_results.append(mc_result)\n",
    "    \n",
    "    # Evaluate DQN\n",
    "    env_dqn = DynamicPricingEnv()  # Fresh environment\n",
    "    dqn_result = evaluate_agent(dqn_agent, env_dqn, num_eval_episodes, seed)\n",
    "    dqn_results.append(dqn_result)\n",
    "    \n",
    "    # Evaluate Double DQN\n",
    "    env_ddqn = DynamicPricingEnv()  # Fresh environment\n",
    "    ddqn_result = evaluate_agent(ddqn_agent, env_ddqn, num_eval_episodes, seed)\n",
    "    ddqn_results.append(ddqn_result)\n",
    "\n",
    "# Aggregate results across seeds\n",
    "def aggregate_results(results_list):\n",
    "    \"\"\"Aggregate evaluation results across seeds.\"\"\"\n",
    "    mean_rewards = [np.mean(r['rewards']) for r in results_list]\n",
    "    return {\n",
    "        'mean_reward': np.mean(mean_rewards),\n",
    "        'std_reward': np.std(mean_rewards),\n",
    "        'all_rewards': mean_rewards\n",
    "    }\n",
    "\n",
    "mc_agg = aggregate_results(mc_results)\n",
    "dqn_agg = aggregate_results(dqn_results)\n",
    "ddqn_agg = aggregate_results(ddqn_results)\n",
    "\n",
    "# Print aggregated results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"Monte Carlo: ${mc_agg['mean_reward']:.2f} \u00b1 ${mc_agg['std_reward']:.2f}\")\n",
    "print(f\"DQN: ${dqn_agg['mean_reward']:.2f} \u00b1 ${dqn_agg['std_reward']:.2f}\")\n",
    "print(f\"Double DQN: ${ddqn_agg['mean_reward']:.2f} \u00b1 ${ddqn_agg['std_reward']:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Analysis Visualization\n",
    "\n",
    "Let's create a visualization comparing the performance of all agents:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot comparative results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Prepare data\n",
    "labels = ['Monte Carlo', 'DQN', 'Double DQN']\n",
    "means = [mc_agg['mean_reward'], dqn_agg['mean_reward'], ddqn_agg['mean_reward']]\n",
    "stds = [mc_agg['std_reward'], dqn_agg['std_reward'], ddqn_agg['std_reward']]\n",
    "\n",
    "# Bar plot with error bars\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['blue', 'green', 'orange'])\n",
    "plt.title('Agent Performance Comparison')\n",
    "plt.ylabel('Average Total Reward ($)')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(means):\n",
    "    plt.text(i, v + stds[i] + 10, f'${v:.2f}', ha='center')\n",
    "\n",
    "plt.savefig('notebook_results/visualizations/performance_comparison.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis of Agent Behavior\n",
    "\n",
    "Now let's examine how the agents behave in more detail, focusing on pricing strategies, market share, and adaptation to seasonal patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's visualize the pricing strategies of each agent over time\n",
    "def visualize_agent_behavior(agent, agent_name, env, num_episodes=3, seed=42):\n",
    "    \"\"\"Visualize agent behavior over multiple episodes.\"\"\"\n",
    "    # Set seed\n",
    "    set_seeds(seed)\n",
    "    \n",
    "    # Run episodes and collect data\n",
    "    all_prices = []\n",
    "    all_competitor_prices = []\n",
    "    all_rewards = []\n",
    "    all_market_shares = []\n",
    "    all_days = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        day = 0\n",
    "        prices = []\n",
    "        competitor_prices = []\n",
    "        rewards = []\n",
    "        market_shares = []\n",
    "        days = []\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Collect data\n",
    "            prices.append(info['price'])\n",
    "            competitor_prices.append(info['competitor_price'])\n",
    "            rewards.append(reward)\n",
    "            market_shares.append(info['market_share'])\n",
    "            days.append(day)\n",
    "            \n",
    "            state = next_state\n",
    "            day += 1\n",
    "            \n",
    "            if day >= env.episode_length:\n",
    "                break\n",
    "        \n",
    "        all_prices.extend(prices)\n",
    "        all_competitor_prices.extend(competitor_prices)\n",
    "        all_rewards.extend(rewards)\n",
    "        all_market_shares.extend(market_shares)\n",
    "        all_days.extend(days)\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Plot prices vs competitor prices\n",
    "    axes[0].plot(all_prices, 'o-', label='Our Price')\n",
    "    axes[0].plot(all_competitor_prices, 's-', label='Competitor Price')\n",
    "    axes[0].set_title(f'{agent_name}: Pricing Strategy vs Competitor')\n",
    "    axes[0].set_xlabel('Day')\n",
    "    axes[0].set_ylabel('Price ($)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot rewards\n",
    "    axes[1].bar(range(len(all_rewards)), all_rewards)\n",
    "    axes[1].set_title(f'{agent_name}: Daily Profit')\n",
    "    axes[1].set_xlabel('Day')\n",
    "    axes[1].set_ylabel('Profit ($)')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Plot market share\n",
    "    axes[2].plot(all_market_shares, 'o-', color='purple')\n",
    "    axes[2].set_title(f'{agent_name}: Market Share')\n",
    "    axes[2].set_xlabel('Day')\n",
    "    axes[2].set_ylabel('Market Share')\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'notebook_results/visualizations/{agent_name.lower().replace(\" \", \"_\")}_behavior.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'prices': all_prices,\n",
    "        'competitor_prices': all_competitor_prices,\n",
    "        'rewards': all_rewards,\n",
    "        'market_shares': all_market_shares\n",
    "    }\n",
    "\n",
    "# Visualize each agent's behavior\n",
    "print(\"Analyzing Monte Carlo agent behavior...\")\n",
    "mc_behavior = visualize_agent_behavior(mc_agent, \"Monte Carlo\", DynamicPricingEnv())\n",
    "\n",
    "print(\"Analyzing DQN agent behavior...\")\n",
    "dqn_behavior = visualize_agent_behavior(dqn_agent, \"DQN\", DynamicPricingEnv())\n",
    "\n",
    "print(\"Analyzing Double DQN agent behavior...\")\n",
    "ddqn_behavior = visualize_agent_behavior(ddqn_agent, \"Double DQN\", DynamicPricingEnv())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality Adaptation Analysis\n",
    "\n",
    "Let's see how well each agent adapts to seasonal patterns:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_seasonality_adaptation(agent, agent_name, env, num_episodes=5):\n",
    "    \"\"\"Analyze how the agent adapts to seasonal patterns.\"\"\"\n",
    "    # Set up data storage\n",
    "    weekend_prices = []\n",
    "    weekday_prices = []\n",
    "    weekend_profits = []\n",
    "    weekday_profits = []\n",
    "    \n",
    "    # Run several episodes\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        day = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Check if weekend (day 5, 6 in our 7-day cycle)\n",
    "            is_weekend = day % 7 >= 5\n",
    "            \n",
    "            # Store data\n",
    "            if is_weekend:\n",
    "                weekend_prices.append(info['price'])\n",
    "                weekend_profits.append(reward)\n",
    "            else:\n",
    "                weekday_prices.append(info['price'])\n",
    "                weekday_profits.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            day += 1\n",
    "            \n",
    "            if day >= env.episode_length:\n",
    "                break\n",
    "    \n",
    "    # Return seasonal data\n",
    "    return {\n",
    "        'weekend_prices': weekend_prices,\n",
    "        'weekday_prices': weekday_prices,\n",
    "        'weekend_profits': weekend_profits,\n",
    "        'weekday_profits': weekday_profits,\n",
    "        'weekend_avg_price': np.mean(weekend_prices),\n",
    "        'weekday_avg_price': np.mean(weekday_prices),\n",
    "        'weekend_avg_profit': np.mean(weekend_profits),\n",
    "        'weekday_avg_profit': np.mean(weekday_profits)\n",
    "    }\n",
    "\n",
    "# Analyze seasonality adaptation for each agent\n",
    "mc_season = analyze_seasonality_adaptation(mc_agent, \"Monte Carlo\", DynamicPricingEnv())\n",
    "dqn_season = analyze_seasonality_adaptation(dqn_agent, \"DQN\", DynamicPricingEnv())\n",
    "ddqn_season = analyze_seasonality_adaptation(ddqn_agent, \"Double DQN\", DynamicPricingEnv())\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot average pricing strategy on weekdays vs weekends\n",
    "agents = ['Monte Carlo', 'DQN', 'Double DQN']\n",
    "weekday_prices = [mc_season['weekday_avg_price'], dqn_season['weekday_avg_price'], ddqn_season['weekday_avg_price']]\n",
    "weekend_prices = [mc_season['weekend_avg_price'], dqn_season['weekend_avg_price'], ddqn_season['weekend_avg_price']]\n",
    "\n",
    "x = np.arange(len(agents))\n",
    "width = 0.35\n",
    "\n",
    "# Bar plot for pricing\n",
    "axes[0].bar(x - width/2, weekday_prices, width, label='Weekday Pricing')\n",
    "axes[0].bar(x + width/2, weekend_prices, width, label='Weekend Pricing')\n",
    "axes[0].set_title('Pricing Strategy Adaptation to Seasonality')\n",
    "axes[0].set_ylabel('Average Price ($)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(agents)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, axis='y')\n",
    "\n",
    "# Bar plot for profits\n",
    "weekday_profits = [mc_season['weekday_avg_profit'], dqn_season['weekday_avg_profit'], ddqn_season['weekday_avg_profit']]\n",
    "weekend_profits = [mc_season['weekend_avg_profit'], dqn_season['weekend_avg_profit'], ddqn_season['weekend_avg_profit']]\n",
    "\n",
    "axes[1].bar(x - width/2, weekday_profits, width, label='Weekday Profit')\n",
    "axes[1].bar(x + width/2, weekend_profits, width, label='Weekend Profit')\n",
    "axes[1].set_title('Profit Performance Across Seasonal Patterns')\n",
    "axes[1].set_ylabel('Average Profit ($)')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(agents)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebook_results/visualizations/seasonality_adaptation.png')\n",
    "plt.show()\n",
    "\n",
    "# Print seasonality insights\n",
    "print(\"Seasonality Adaptation Insights:\")\n",
    "for agent_name, season_data in [\n",
    "    (\"Monte Carlo\", mc_season),\n",
    "    (\"DQN\", dqn_season),\n",
    "    (\"Double DQN\", ddqn_season)\n",
    "]:\n",
    "    price_diff = season_data['weekend_avg_price'] - season_data['weekday_avg_price']\n",
    "    profit_diff = season_data['weekend_avg_profit'] - season_data['weekday_avg_profit']\n",
    "    \n",
    "    print(f\"\\n{agent_name}:\")\n",
    "    print(f\"  Weekday vs Weekend Price Difference: ${price_diff:.2f} ({price_diff/season_data['weekday_avg_price']*100:.1f}%)\")\n",
    "    print(f\"  Weekday vs Weekend Profit Difference: ${profit_diff:.2f} ({profit_diff/season_data['weekday_avg_profit']*100:.1f}%)\")\n",
    "    if price_diff > 0:\n",
    "        print(f\"  Strategy: Increases prices on weekends to capitalize on higher demand\")\n",
    "    elif price_diff < 0:\n",
    "        print(f\"  Strategy: Decreases prices on weekends to capture more market share\")\n",
    "    else:\n",
    "        print(f\"  Strategy: Maintains consistent pricing regardless of day\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Key Insights\n",
    "\n",
    "In this tutorial, we've implemented and compared three reinforcement learning algorithms for dynamic pricing:\n",
    "\n",
    "1. **Monte Carlo** - Learning directly from complete episodes\n",
    "2. **DQN (Deep Q-Network)** - Using experience replay and target networks\n",
    "3. **Double DQN** - Separating action selection and evaluation to reduce bias\n",
    "\n",
    "Let's summarize the key insights from our comparison:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a summary comparison table\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data\n",
    "comparison_data = {\n",
    "    'Algorithm': ['Monte Carlo', 'DQN', 'Double DQN'],\n",
    "    'Avg Reward': [mc_agg['mean_reward'], dqn_agg['mean_reward'], ddqn_agg['mean_reward']],\n",
    "    'Std Deviation': [mc_agg['std_reward'], dqn_agg['std_reward'], ddqn_agg['std_reward']],\n",
    "    'Weekend/Weekday Price Diff': [\n",
    "        mc_season['weekend_avg_price'] - mc_season['weekday_avg_price'],\n",
    "        dqn_season['weekend_avg_price'] - dqn_season['weekday_avg_price'],\n",
    "        ddqn_season['weekend_avg_price'] - ddqn_season['weekday_avg_price']\n",
    "    ],\n",
    "    'Avg Market Share': [\n",
    "        np.mean(mc_behavior['market_shares']),\n",
    "        np.mean(dqn_behavior['market_shares']),\n",
    "        np.mean(ddqn_behavior['market_shares'])\n",
    "    ],\n",
    "    'Avg Price': [\n",
    "        np.mean(mc_behavior['prices']),\n",
    "        np.mean(dqn_behavior['prices']),\n",
    "        np.mean(ddqn_behavior['prices'])\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Format for display\n",
    "comparison_df['Avg Reward'] = comparison_df['Avg Reward'].map('${:.2f}'.format)\n",
    "comparison_df['Std Deviation'] = comparison_df['Std Deviation'].map('${:.2f}'.format)\n",
    "comparison_df['Weekend/Weekday Price Diff'] = comparison_df['Weekend/Weekday Price Diff'].map('${:.2f}'.format)\n",
    "comparison_df['Avg Market Share'] = comparison_df['Avg Market Share'].map('{:.2%}'.format)\n",
    "comparison_df['Avg Price'] = comparison_df['Avg Price'].map('${:.2f}'.format)\n",
    "\n",
    "# Display comparison table\n",
    "comparison_df.set_index('Algorithm', inplace=True)\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insights\n",
    "\n",
    "From our experiments, we can draw several key business insights:\n",
    "\n",
    "1. **Pricing Strategy Impact**:\n",
    "   - All three algorithms learned to adjust pricing based on both competitor prices and inventory levels\n",
    "   - Double DQN typically achieved the highest profit by better balancing profit margins and market share\n",
    "\n",
    "2. **Seasonal Adaptation**:\n",
    "   - The agents learned to adjust prices for weekends vs. weekdays\n",
    "   - Higher prices on weekends generally yielded better profits due to increased demand\n",
    "\n",
    "3. **Competitive Response**:\n",
    "   - All agents learned to strategically position their prices relative to competitors\n",
    "   - When competitors price low, the optimal strategy typically involves either matching to maintain market share or pricing higher to maintain margins\n",
    "\n",
    "4. **Inventory Management**:\n",
    "   - Agents learned to adjust pricing to optimize inventory allocation across the selling period\n",
    "   - As inventory decreases, prices typically increase to maximize profit from remaining units\n",
    "\n",
    "5. **Algorithm Differences**:\n",
    "   - Monte Carlo: More stable but slower to adapt to changing conditions\n",
    "   - DQN: Faster learning but occasionally unstable\n",
    "   - Double DQN: Best overall performance with more consistent behavior in dynamic environments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Comparison of Algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a radar chart to compare algorithms across multiple dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Categories for comparison\n",
    "categories = ['Profit', 'Stability', 'Learning Speed', 'Seasonal Adaptation', 'Competitive Response']\n",
    "N = len(categories)\n",
    "\n",
    "# Values for each algorithm (subjective ratings based on our observations)\n",
    "# Scale: 0-5 where 5 is best\n",
    "values_mc = [3.5, 4.0, 2.5, 3.0, 3.0]\n",
    "values_dqn = [4.0, 3.0, 4.0, 3.5, 3.5]\n",
    "values_ddqn = [4.5, 4.0, 4.0, 4.0, 4.0]\n",
    "\n",
    "# Angle of each axis\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Close the loop\n",
    "\n",
    "# Values for each algorithm (extended to close the loop)\n",
    "values_mc += values_mc[:1]\n",
    "values_dqn += values_dqn[:1]\n",
    "values_ddqn += values_ddqn[:1]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Add each algorithm\n",
    "ax.plot(angles, values_mc, 'o-', linewidth=2, label='Monte Carlo')\n",
    "ax.plot(angles, values_dqn, 's-', linewidth=2, label='DQN')\n",
    "ax.plot(angles, values_ddqn, '^-', linewidth=2, label='Double DQN')\n",
    "ax.fill(angles, values_mc, alpha=0.1)\n",
    "ax.fill(angles, values_dqn, alpha=0.1)\n",
    "ax.fill(angles, values_ddqn, alpha=0.1)\n",
    "\n",
    "# Fix axis to go in the right order and start at 12 o'clock\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "\n",
    "# Draw axis lines for each angle and label\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "\n",
    "# Draw ylabels\n",
    "ax.set_rlabel_position(0)\n",
    "ax.set_yticks([1, 2, 3, 4, 5])\n",
    "ax.set_yticklabels(['1', '2', '3', '4', '5'])\n",
    "ax.set_ylim(0, 5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "plt.title('Algorithm Comparison Across Key Dimensions', size=15, y=1.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebook_results/visualizations/algorithm_radar_comparison.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways for Dynamic Pricing with RL\n",
    "\n",
    "1. **State Representation Matters**: Including competitor prices, inventory levels, and seasonality in the state space is crucial for developing effective pricing strategies.\n",
    "\n",
    "2. **Exploration-Exploitation Balance**: Setting appropriate epsilon decay rates helps balance exploration of different pricing strategies with exploitation of known profitable strategies.\n",
    "\n",
    "3. **Algorithm Selection**: \n",
    "   - For stable environments with less frequent changes: Monte Carlo may be sufficient\n",
    "   - For highly dynamic environments: Double DQN typically performs best\n",
    "   - For a balance of performance and simplicity: DQN is a good starting point\n",
    "\n",
    "4. **Competitor Modeling**: How we model competitor behavior significantly impacts the learned pricing strategy. More sophisticated competitor models would yield more realistic strategies.\n",
    "\n",
    "5. **Business Constraints**: Incorporating business constraints (min/max prices, inventory thresholds) into the reward function helps learn practical pricing policies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions and Future Work\n",
    "\n",
    "This tutorial provides a foundation for applying RL to dynamic pricing. Here are some potential extensions:\n",
    "\n",
    "1. **Multi-product Pricing**: Extend to price multiple products with cross-product demand effects\n",
    "\n",
    "2. **Customer Segmentation**: Add different customer segments with varying price sensitivities\n",
    "\n",
    "3. **Long-term Effects**: Model how pricing affects customer loyalty and lifetime value\n",
    "\n",
    "4. **Advanced RL Algorithms**: Implement PPO, SAC, or other state-of-the-art algorithms\n",
    "\n",
    "5. **Real-world Application**: Connect to actual e-commerce data for training and validation\n",
    "\n",
    "6. **UI for Business Users**: Create an interactive dashboard for non-technical users to apply RL pricing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercise: Testing Different Business Scenarios\n",
    "\n",
    "As a final exercise, let's see how our best agent (Double DQN) performs under different business scenarios:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def test_business_scenario(scenario_name, env_params, agent):\n",
    "    \"\"\"Test agent performance under different business scenarios.\"\"\"\n",
    "    # Create environment with specified parameters\n",
    "    env = DynamicPricingEnv(**env_params)\n",
    "    \n",
    "    # Evaluate agent\n",
    "    results = evaluate_agent(agent, env, num_episodes=5, seed=42)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{scenario_name} Scenario:\")\n",
    "    print(f\"  Average Profit: ${np.mean(results['rewards']):.2f}\")\n",
    "    print(f\"  Average Market Share: {np.mean([np.mean(ms) for ms in results['market_shares']]):.2%}\")\n",
    "    \n",
    "    # Return results for further analysis\n",
    "    return results\n",
    "\n",
    "# Define different business scenarios\n",
    "scenarios = {\n",
    "    \"High Competition\": {\n",
    "        \"competitor_prices\": [12.99, 19.99],  # More aggressive competitor pricing\n",
    "        \"market_share_split\": 0.5  # Harder to capture market share\n",
    "    },\n",
    "    \"Low Elasticity\": {\n",
    "        \"elasticity\": 0.3  # Customers less sensitive to price\n",
    "    },\n",
    "    \"High Elasticity\": {\n",
    "        \"elasticity\": 0.8  # Customers more sensitive to price\n",
    "    },\n",
    "    \"Limited Inventory\": {\n",
    "        \"inventory\": 500  # Half the normal inventory\n",
    "    },\n",
    "    \"High Seasonality\": {\n",
    "        \"seasonality_factors\": {'weekend': 1.5, 'holiday': 2.0}  # Stronger seasonal effects\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test each scenario with our best agent (Double DQN)\n",
    "scenario_results = {}\n",
    "for name, params in scenarios.items():\n",
    "    scenario_results[name] = test_business_scenario(name, params, ddqn_agent)\n",
    "\n",
    "# Visualize scenario performance comparison\n",
    "scenario_names = list(scenarios.keys())\n",
    "scenario_profits = [np.mean(scenario_results[name]['rewards']) for name in scenario_names]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(scenario_names, scenario_profits)\n",
    "plt.title('Double DQN Performance Across Business Scenarios')\n",
    "plt.ylabel('Average Profit ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "            f'${height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebook_results/visualizations/scenario_comparison.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored how reinforcement learning can be applied to dynamic pricing problems. We've implemented and compared three different RL algorithms, analyzed their performance across various business metrics, and tested them in different scenarios.\n",
    "\n",
    "The results demonstrate that RL can effectively learn pricing strategies that adapt to:\n",
    "- Competitor behavior\n",
    "- Seasonal demand patterns\n",
    "- Inventory constraints\n",
    "- Different market conditions\n",
    "\n",
    "While Double DQN generally performed best in our experiments, the optimal algorithm choice depends on your specific business context and requirements. The framework provided in this notebook can be extended and customized to address the unique challenges of your pricing environment.\n",
    "\n",
    "We hope this tutorial provides a solid foundation for applying reinforcement learning to your own dynamic pricing challenges!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
